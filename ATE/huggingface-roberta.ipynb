{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0ff259",
   "metadata": {
    "id": "2736d24a-f23c-4bfc-9ed2-7b8a91399109"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "TRAIN_FILE = './data/train_set_v2.csv'\n",
    "TASK2_FILE = './data/test_task2.csv'\n",
    "EXTERNAL_FILE = './data/external-data.csv'\n",
    "TEST_FILE = './data/test_set_v2.csv'\n",
    "SPLIT_TRAIN_IN_VAL = False\n",
    "\n",
    "modelo = 'diiogo/beto52'\n",
    "use_auth_token = None\n",
    "MAX_LEN = 512\n",
    "experiment_name = 'roberta-final'\n",
    "model_dir ='./ATE/expreriments/' + experiment_name + '/'\n",
    "\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE, sep=';')\n",
    "task2_df = pd.read_csv(TASK2_FILE, sep=';')\n",
    "external_df = pd.read_csv(EXTERNAL_FILE, sep=';')\n",
    "test_df = pd.read_csv(TEST_FILE, sep=';')\n",
    "\n",
    "ate_train_data = train_df.groupby('review').agg(list).reset_index()\n",
    "ate_task2_data = task2_df.groupby('review').agg(list).reset_index()\n",
    "\n",
    "#ate_train_data = pd.concat((ate_train_data, ate_task2_data), ignore_index=True)\n",
    "\n",
    "ate_test_data = test_df.groupby('review').agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e64693",
   "metadata": {
    "id": "a905eaf8-4426-4cb8-a65d-78e198ef1c23",
    "outputId": "89ef5188-31dc-4778-ed0a-8a0333da2d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=ABSAPT22_ATE\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_NOTEBOOK_NAME=experiment_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find experiment_name.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meduagarcia\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uso_no_colab = False\n",
    "if uso_no_colab:\n",
    "  !pip install transformers datasets seqeval\n",
    "  !pip install wandb\n",
    "\n",
    "%env WANDB_PROJECT=ABSAPT22_ATE\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_NOTEBOOK_NAME=experiment_name    \n",
    "    \n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e632f298",
   "metadata": {
    "id": "e415e227-c13a-41e1-8cc3-8f849e78fbc6"
   },
   "outputs": [],
   "source": [
    "if SPLIT_TRAIN_IN_VAL:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    ate_train_data, ate_dev_data = train_test_split(ate_train_data, test_size=0.2)\n",
    "else:\n",
    "    ate_dev_data = ate_test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d904c79",
   "metadata": {
    "id": "273adde0-5d02-404b-804a-4cb95c006dbd"
   },
   "outputs": [],
   "source": [
    "def is_span_a_subset(span, aspect_span):\n",
    "    if span[0] >= aspect_span[1]:\n",
    "        return False\n",
    "    elif span[1] < aspect_span[0]:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec67250",
   "metadata": {
    "id": "021cdb2f-ecc3-47fb-917b-fdb3891e8a65",
    "outputId": "11758730-b654-4687-8ec0-e70ee8528e19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer   \n",
    "def convert_to_bio(df):\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        #tokens = tokenize.word_tokenize(row['review'], language='portuguese')\n",
    "        aspects_span = [[i, j, 0] for i, j in zip(row['start_position'], row['end_position'])]\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        span_generator = TreebankWordTokenizer().span_tokenize(row['review'])\n",
    "        for span in span_generator:\n",
    "            tokens.append(row['review'][span[0]:span[1]])\n",
    "            is_aspect = False\n",
    "            aspect_data = None\n",
    "            for aspect_span in aspects_span:\n",
    "                if is_span_a_subset(span, aspect_span):\n",
    "                    is_aspect = True\n",
    "                    aspect_data = aspect_span\n",
    "            if is_aspect:\n",
    "                label = 'ASPECT'\n",
    "                \n",
    "                #polarity_id = int(aspect_data[2])\n",
    "                #if polarity_id == 1:\n",
    "                #    label = 'POSITIVE'\n",
    "                #elif polarity_id == 0:\n",
    "                #    label = 'NEUTRAL'\n",
    "                #else:\n",
    "                #    label = 'NEGATIVE'\n",
    "                \n",
    "                if aspect_data[-1] == 0:\n",
    "                    ner_tags.append('B-'+label)\n",
    "                    aspect_data[-1] = aspect_data[-1] + 1\n",
    "                else:\n",
    "                    ner_tags.append('I-'+label)\n",
    "            else:\n",
    "                ner_tags.append('O')\n",
    "        data.append({'id': i, 'tokens': tokens, 'ner_tags': ner_tags})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353cdae9",
   "metadata": {
    "id": "e728fef0-0105-4a1c-9cc4-081cc38d65c2"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data = {\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(convert_to_bio(ate_train_data))),\n",
    "    'validation':  Dataset.from_pandas(pd.DataFrame(convert_to_bio(ate_dev_data))),\n",
    "    'test':  Dataset.from_pandas(pd.DataFrame(convert_to_bio(ate_test_data)))\n",
    "}\n",
    "\n",
    "\n",
    "dataset = datasets.DatasetDict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b349b635",
   "metadata": {
    "id": "d110e046-b053-457b-ac67-ad6cee13b321"
   },
   "outputs": [],
   "source": [
    "#sep = \" \"\n",
    "#for data_type in dataset.keys():\n",
    "#    with open(f'./ATE/data/aspect-final/{data_type}.conll', 'w') as f:\n",
    "#        for tokens, tags in zip(dataset[data_type]['tokens'], dataset[data_type]['ner_tags']):\n",
    "#            for token, tag in zip(tokens, tags):\n",
    "#                f.write(str(token)+sep+tag+'\\n')\n",
    "#            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19a1d87",
   "metadata": {
    "id": "6360aca6-1f4b-4dff-8c3e-e75083aabaa6",
    "outputId": "76bdfb38-7126-47e2-cf56-c7b63dbf9434"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-ASPECT', 'B-ASPECT', 'O']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(set(tag for doc in dataset['train']['ner_tags'] for tag in doc))\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ff3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['B-ASPECT', 'I-ASPECT', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec255bb",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "425536fa452144d3bb683b143918467e",
      "6750a11e217d4463b09a0d89462f9ce1",
      "0ad199038d284a4a9f7315599d52e87e"
     ]
    },
    "id": "b5feee15-3280-4e94-988c-d3794bba969d",
    "outputId": "d07d4267-68cc-425d-e019-8dcee11619c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 677/677 [00:00<00:00, 2020.71ex/s]\n",
      "100%|██████████| 170/170 [00:00<00:00, 2053.96ex/s]\n",
      "100%|██████████| 170/170 [00:00<00:00, 2039.11ex/s]\n"
     ]
    }
   ],
   "source": [
    "features = datasets.Features(\n",
    "    {\n",
    "        'id': datasets.Value('int32'),\n",
    "        'tokens': datasets.Sequence(datasets.Value('string')),\n",
    "        'ner_tags': datasets.Sequence(\n",
    "            datasets.features.ClassLabel(names=label_list)\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.map(features.encode_example, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20d3f81",
   "metadata": {
    "id": "e82384f0-4636-4cd5-976a-d38964c46264"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo, use_auth_token=use_auth_token, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff798dc7",
   "metadata": {
    "id": "4e372509-0246-4e25-927f-f029cd5566e9"
   },
   "outputs": [],
   "source": [
    "#função que ajusta os labels para o tamanho dos textos após tokenização do BERT\n",
    "#necessário pois palavras podem ser subdivididas com ##\n",
    "def tokenize_and_align_labels(dataset_unaligned, label_all_tokens = False):\n",
    "    tokenized_inputs = tokenizer(dataset_unaligned[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
    "    #print(tokenized_inputs)\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset_unaligned[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # None é o valor para tokens especiais -> -100 para ignorar na função de custo\n",
    "            if word_idx is None: #special tokens\n",
    "                label_ids.append(-100)\n",
    "            #palavra nova\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            #label_all_tokens = True -> mesma tag para todos os subtokens\n",
    "            #label_all_tokens = False -> apenas primeiro subtoken ganha tag\n",
    "            else: #subpalavra\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92732607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#função que ajusta os labels para o tamanho dos textos após tokenização do BERT\n",
    "#necessário pois palavras podem ser subdivididas com ##\n",
    "def tokenize_and_align_labels_python_tokenizer(dataset_unaligned, label_all_tokens = False):    \n",
    "    tokenized_inputs = tokenizer(dataset_unaligned[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
    "    special_tokens = [tokenized_inputs['input_ids'][0][0], tokenized_inputs['input_ids'][0][-1]]\n",
    "    \n",
    "    word_ids_batch = []\n",
    "    for tokens in dataset_unaligned[\"tokens\"]:\n",
    "        word_ids_batch.append([None])\n",
    "        tokens_len = 2\n",
    "        for word_idx, token in enumerate(tokens):\n",
    "            token_processed = tokenizer(token, add_special_tokens=False, truncation=True, is_split_into_words=True, max_length=512)\n",
    "            for i in range(len(token_processed['input_ids'])):\n",
    "                tokens_len += 1\n",
    "                if tokens_len <= 512:\n",
    "                    word_ids_batch[-1].append(word_idx)\n",
    "        word_ids_batch[-1].append(None)\n",
    "    #print(tokenized_inputs)\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset_unaligned[f\"ner_tags\"]):\n",
    "        word_ids = word_ids_batch[i]\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # None é o valor para tokens especiais -> -100 para ignorar na função de custo\n",
    "            if word_idx is None: #special tokens\n",
    "                label_ids.append(-100)\n",
    "            #palavra nova\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            #label_all_tokens = True -> mesma tag para todos os subtokens\n",
    "            #label_all_tokens = False -> apenas primeiro subtoken ganha tag\n",
    "            else: #subpalavra\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b20e00",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "64fc09e89a3d4301b34fd608c8babf90",
      "b875f04cfbf6411cb57d9141fb2c1657",
      "af2e200cfd9f42399ca0fa01899c1ff1"
     ]
    },
    "id": "ea1b2046-0a66-48c3-98cb-8734aadc184e",
    "outputId": "5cfa0f0c-d24b-4ec9-d799-d60b30dfe117"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.55ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.15ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.22ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 677\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ed94c3",
   "metadata": {
    "id": "b2f0ccf3-3dbb-4d77-a732-120b595bd82f"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5215df",
   "metadata": {
    "id": "c9048111-a445-4459-86ee-3c56b83c6bb7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5b9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6f6413",
   "metadata": {
    "id": "b6a9d1bf-9dd4-4e48-b93b-c1283e76c311",
    "outputId": "8945a6c2-d9d7-4b86-c0f5-5970eb796e5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pytorch pre-release version 1.9.0a0+2ecb2c7 - assuming intent to test it\n",
      "Some weights of the model checkpoint at diiogo/beto52 were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at diiogo/beto52 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(modelo, num_labels=len(label_list), use_auth_token=use_auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d2e99a2",
   "metadata": {
    "id": "8325e636-af90-448c-96fc-b30d67707e6e"
   },
   "outputs": [],
   "source": [
    "per_device_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "total_steps_epoch = len(dataset['train']) // (per_device_batch_size * gradient_accumulation_steps)\n",
    "\n",
    "learning_rate = 3e-5 \n",
    "num_train_epochs = 10  \n",
    "weight_decay = 0.1\n",
    "warmup_ratio=0.1                         #  primeiros 10% --> Artigo Souza 2019\n",
    "warmup_steps=1000\n",
    "\n",
    "save_total_limit = 3\n",
    "logging_steps = total_steps_epoch#(num_train_epochs*total_steps_epoch) // 20 # 20x por treinamento\n",
    "eval_steps = logging_steps\n",
    "evaluation_strategy = 'steps'\n",
    "logging_strategy = 'steps'\n",
    "save_strategy = 'steps'\n",
    "save_steps = logging_steps\n",
    "load_best_model_at_end = True\n",
    "\n",
    "fp16 = False\n",
    "\n",
    "# folders\n",
    "\n",
    "folder_model = 'e' + str(num_train_epochs) + '_lr' + str(learning_rate)\n",
    "output_dir = model_dir + 'results'\n",
    "logging_dir = model_dir + 'results'\n",
    "# get best model through a metric\n",
    "metric_for_best_model = 'eval_f1'\n",
    "if metric_for_best_model == 'eval_f1':\n",
    "    greater_is_better = True\n",
    "elif metric_for_best_model == 'eval_loss':\n",
    "    greater_is_better = False  \n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size*2,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    #warmup_ratio=warmup_ratio,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps = logging_steps,\n",
    "    eval_steps = logging_steps,\n",
    "    load_best_model_at_end = load_best_model_at_end,\n",
    "    metric_for_best_model = metric_for_best_model,\n",
    "    greater_is_better = greater_is_better,\n",
    "    gradient_checkpointing = False,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True,\n",
    "    evaluation_strategy = evaluation_strategy,\n",
    "    logging_dir=logging_dir, \n",
    "    logging_strategy = logging_strategy,\n",
    "    save_strategy = save_strategy,\n",
    "    save_steps = save_steps,\n",
    "    fp16 = fp16,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d2b5a59",
   "metadata": {
    "id": "c8ad20a7-a759-4376-8278-33f535e8e2e4"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ffea589",
   "metadata": {
    "id": "64ba72c7-a0a4-48d1-b411-8b9d0b93646c"
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "# wait early_stopping_patience x eval_steps before to stop the training in order to get a better model\n",
    "early_stopping_patience = save_total_limit\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbac2a8f",
   "metadata": {
    "id": "a8458ef0-8f5b-4211-b202-42045e95cfef",
    "outputId": "5dc8029b-645c-49db-cf16-edd0c2323b65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 677\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 850\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/repos/ASLB/wandb/run-20220507_015832-2ohv6dw9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eduagarcia/ABSAPT22_ATE/runs/2ohv6dw9\" target=\"_blank\">./ATE/expreriments/roberta-final/results</a></strong> to <a href=\"https://wandb.ai/eduagarcia/ABSAPT22_ATE\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [850/850 02:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.927600</td>\n",
       "      <td>0.266217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.181300</td>\n",
       "      <td>0.116771</td>\n",
       "      <td>0.458155</td>\n",
       "      <td>0.706954</td>\n",
       "      <td>0.555990</td>\n",
       "      <td>0.952662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.086293</td>\n",
       "      <td>0.604230</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>0.631912</td>\n",
       "      <td>0.969411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.076220</td>\n",
       "      <td>0.688586</td>\n",
       "      <td>0.918874</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.975073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.913907</td>\n",
       "      <td>0.808199</td>\n",
       "      <td>0.978611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.053444</td>\n",
       "      <td>0.790393</td>\n",
       "      <td>0.899007</td>\n",
       "      <td>0.841208</td>\n",
       "      <td>0.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.057138</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.928808</td>\n",
       "      <td>0.828656</td>\n",
       "      <td>0.980813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.052318</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.920530</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.051995</td>\n",
       "      <td>0.802671</td>\n",
       "      <td>0.895695</td>\n",
       "      <td>0.846635</td>\n",
       "      <td>0.983644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.064899</td>\n",
       "      <td>0.823105</td>\n",
       "      <td>0.754967</td>\n",
       "      <td>0.787565</td>\n",
       "      <td>0.979476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-84\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-84/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-84/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-84/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-84/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-168\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-168/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-168/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-168/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-168/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-252\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-252/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-252/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-252/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-252/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-336\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-336/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-336/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-336/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-336/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-84] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-420\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-420/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-420/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-420/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-168] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-504\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-504/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-504/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-504/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-504/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-252] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-588\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-588/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-588/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-588/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-588/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-336] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-672\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-672/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-672/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-672/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-672/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-420] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-756\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-756/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-756/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-756/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-756/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-504] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/results/checkpoint-840\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/results/checkpoint-840/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/results/checkpoint-840/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/results/checkpoint-840/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/results/checkpoint-840/special_tokens_map.json\n",
      "Deleting older checkpoint [ATE/expreriments/roberta-final/results/checkpoint-588] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./ATE/expreriments/roberta-final/results/checkpoint-672 (score: 0.846915460776847).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=850, training_loss=0.1573619868474848, metrics={'train_runtime': 134.5225, 'train_samples_per_second': 50.326, 'train_steps_per_second': 6.319, 'total_flos': 429922789699536.0, 'train_loss': 0.1573619868474848, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53bbf7d9",
   "metadata": {
    "id": "72aa8dc1-2c96-4c29-b556-42fbcd7dfc3d",
    "outputId": "39889d7c-348d-479d-fb50-5ffaad40a82d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05231776088476181,\n",
       " 'eval_precision': 0.7842031029619182,\n",
       " 'eval_recall': 0.9205298013245033,\n",
       " 'eval_f1': 0.846915460776847,\n",
       " 'eval_accuracy': 0.9830148619957537,\n",
       " 'eval_runtime': 0.4289,\n",
       " 'eval_samples_per_second': 396.325,\n",
       " 'eval_steps_per_second': 25.645,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37088ed9",
   "metadata": {
    "id": "fd741e29-817f-4cf1-91cd-b1b59848a4dc",
    "outputId": "2a0ae5ad-b53f-4de5-b4a8-1b2f4686878d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ASPECT': {'precision': 0.7842031029619182,\n",
       "  'recall': 0.9205298013245033,\n",
       "  'f1': 0.846915460776847,\n",
       "  'number': 604},\n",
       " 'overall_precision': 0.7842031029619182,\n",
       " 'overall_recall': 0.9205298013245033,\n",
       " 'overall_f1': 0.846915460776847,\n",
       " 'overall_accuracy': 0.9830148619957537}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9347b3d6",
   "metadata": {
    "id": "52ae96db-a44c-49e9-9d63-d5dc1d2eaa28",
    "outputId": "cbdfdf37-e60c-49da-ec48-ae818c451c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./ATE/expreriments/roberta-final/\n",
      "Configuration saved in ./ATE/expreriments/roberta-final/config.json\n",
      "Model weights saved in ./ATE/expreriments/roberta-final/pytorch_model.bin\n",
      "tokenizer config file saved in ./ATE/expreriments/roberta-final/tokenizer_config.json\n",
      "Special tokens file saved in ./ATE/expreriments/roberta-final/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9711f19",
   "metadata": {
    "id": "a456f359-6eec-4155-bd87-c8b5fd50f09a",
    "outputId": "d9d1d1e8-80ee-4f30-ca26-f28096df4472"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASPECT</th>\n",
       "      <th>overall_precision</th>\n",
       "      <th>overall_recall</th>\n",
       "      <th>overall_f1</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>604.000000</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.920530</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ASPECT  overall_precision  overall_recall  overall_f1  \\\n",
       "f1           0.846915           0.784203         0.92053    0.846915   \n",
       "number     604.000000           0.784203         0.92053    0.846915   \n",
       "precision    0.784203           0.784203         0.92053    0.846915   \n",
       "recall       0.920530           0.784203         0.92053    0.846915   \n",
       "\n",
       "           overall_accuracy  \n",
       "f1                 0.983015  \n",
       "number             0.983015  \n",
       "precision          0.983015  \n",
       "recall             0.983015  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "resultado_eval = pd.DataFrame(results)\n",
    "resultado_eval.to_csv(model_dir+'metricas_validation.csv')\n",
    "resultado_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1526ef6",
   "metadata": {
    "id": "e5be9bfe-526d-466e-8a27-9a8131c097b3",
    "outputId": "aa837d0d-4ef6-4d68-f569-808b2aa4bd4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags, id. If tokens, ner_tags, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 170\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ASPECT': {'precision': 0.7842031029619182,\n",
       "  'recall': 0.9205298013245033,\n",
       "  'f1': 0.846915460776847,\n",
       "  'number': 604},\n",
       " 'overall_precision': 0.7842031029619182,\n",
       " 'overall_recall': 0.9205298013245033,\n",
       " 'overall_f1': 0.846915460776847,\n",
       " 'overall_accuracy': 0.9830148619957537}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(raw_predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e4834be",
   "metadata": {
    "id": "415f2358-d5e9-4f77-8acb-9710cb8d6359",
    "outputId": "8b7d4cab-1b9d-48dc-a325-3dd7a19d79eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASPECT</th>\n",
       "      <th>overall_precision</th>\n",
       "      <th>overall_recall</th>\n",
       "      <th>overall_f1</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>604.000000</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.920530</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>0.92053</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ASPECT  overall_precision  overall_recall  overall_f1  \\\n",
       "f1           0.846915           0.784203         0.92053    0.846915   \n",
       "number     604.000000           0.784203         0.92053    0.846915   \n",
       "precision    0.784203           0.784203         0.92053    0.846915   \n",
       "recall       0.920530           0.784203         0.92053    0.846915   \n",
       "\n",
       "           overall_accuracy  \n",
       "f1                 0.983015  \n",
       "number             0.983015  \n",
       "precision          0.983015  \n",
       "recall             0.983015  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado_teste = pd.DataFrame(results)\n",
    "resultado_teste.to_csv(model_dir+'metricas_test.csv')\n",
    "resultado_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "897fe3d0",
   "metadata": {
    "id": "e5cf8e0b-b84f-406c-950b-63e34615ebed"
   },
   "outputs": [],
   "source": [
    "o_index = label_list.index('O')\n",
    "preds_index = np.asarray([i for i in range(len(label_list)) if i != o_index])\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for sentence_raw_prediction, sentence_labels in zip(raw_predictions, true_labels):\n",
    "    for raw_prediction, true_label in zip(sentence_raw_prediction, sentence_labels):\n",
    "        if true_label.startswith('B'):\n",
    "            best_pred_idx = np.argmax(raw_prediction[preds_index])\n",
    "            best_pred = preds_index[best_pred_idx]\n",
    "            pred_label = label_list[best_pred][2:]\n",
    "            true_label = true_label[2:]\n",
    "            y_pred.append(pred_label)\n",
    "            y_true.append(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5985af48",
   "metadata": {
    "id": "e3374b9d-c307-4c2e-bc47-283eae23bdb4",
    "outputId": "551f70a0-8a66-4e7f-8e82-6f275ce4072f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ASPECT       1.00      1.00      1.00       604\n",
      "\n",
      "    accuracy                           1.00       604\n",
      "   macro avg       1.00      1.00      1.00       604\n",
      "weighted avg       1.00      1.00      1.00       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "allenlp-ner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
