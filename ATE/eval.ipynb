{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d313c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6395e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbc1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TASK1 = './data/test_task1.csv'\n",
    "TEST_TASK2 = './data/test_task2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fea2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_MODELS = [ 'roberta-final2',\n",
    "                        'mdeberta-v3-base-huggingface-final2', 'mdeberta-v3-base-huggingface-more-training-portuguese-data-final2' ]\n",
    "for i in range(len(HUGGING_FACE_MODELS)):\n",
    "    HUGGING_FACE_MODELS[i] = os.path.join('./ATE/expreriments', HUGGING_FACE_MODELS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e366d8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./ATE/expreriments/roberta-final2',\n",
       " './ATE/expreriments/mdeberta-v3-base-huggingface-final2',\n",
       " './ATE/expreriments/mdeberta-v3-base-huggingface-more-training-portuguese-data-final2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dadc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TEST_TASK1, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd6871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer  \n",
    "\n",
    "pre_dataset = []\n",
    "for i, row in df.iterrows():\n",
    "    pre_dataset.append({'id': i, 'tokens': TreebankWordTokenizer().tokenize(row['review'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec68eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6c0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:00<00:00, 3303.69ex/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/ba]\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "<ipython-input-9-0a482aca7527>:79: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  models_preds.append(np.array(preds))\n",
      "100%|██████████| 257/257 [00:00<00:00, 2173.75ex/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/ba]\n",
      "100%|██████████| 257/257 [00:00<00:00, 3317.06ex/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/ba]\n"
     ]
    }
   ],
   "source": [
    "models_preds = []\n",
    "for model_path in HUGGING_FACE_MODELS:\n",
    "    if 'roberta' in model_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    data = {\n",
    "        'test':  Dataset.from_pandas(pd.DataFrame(pre_dataset))\n",
    "    }\n",
    "\n",
    "    dataset = datasets.DatasetDict(data)\n",
    "\n",
    "    features = datasets.Features(\n",
    "        {\n",
    "            'id': datasets.Value('int32'),\n",
    "            'tokens': datasets.Sequence(datasets.Value('string'))\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(features.encode_example, features=features)\n",
    "\n",
    "    def tokenize_and_align_labels(dataset_unaligned, label_all_tokens = False):\n",
    "        tokenized_inputs = tokenizer(dataset_unaligned[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
    "\n",
    "        word_ids_batch = []\n",
    "        for tokens in dataset_unaligned[\"tokens\"]:\n",
    "            word_ids_batch.append([None])\n",
    "            tokens_len = 2\n",
    "            for word_idx, token in enumerate(tokens):\n",
    "                if 'roberta' in model_path:\n",
    "                    token_processed = tokenizer([token], add_special_tokens=False, truncation=True, is_split_into_words=True, max_length=512)\n",
    "                else:\n",
    "                    token_processed = tokenizer(token, add_special_tokens=False, truncation=True, is_split_into_words=True, max_length=512)\n",
    "                for i in range(len(token_processed['input_ids'])):\n",
    "                    tokens_len += 1\n",
    "                    if tokens_len <= 512:\n",
    "                        word_ids_batch[-1].append(word_idx)\n",
    "            word_ids_batch[-1].append(None)\n",
    "        tokenized_inputs['word_id'] = word_ids_batch\n",
    "        return tokenized_inputs\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    \n",
    "    raw_input_1 = tokenizer([p['tokens'] for p in pre_dataset[0:125]], pad_to_max_length=True, truncation=True, is_split_into_words=True, max_length=512, return_tensors=\"pt\")\n",
    "    raw_input_1.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores_1 = torch.nn.functional.softmax(model(**raw_input_1).logits.cpu().detach(), dim=1).numpy()\n",
    "    \n",
    "    del raw_input_1\n",
    "    \n",
    "    raw_input = tokenizer([p['tokens'] for p in pre_dataset[125:]], pad_to_max_length=True, truncation=True, is_split_into_words=True, max_length=512, return_tensors=\"pt\")\n",
    "    raw_input.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = torch.nn.functional.softmax(model(**raw_input).logits.cpu().detach(), dim=1).numpy()\n",
    "    \n",
    "    del raw_input\n",
    "    \n",
    "    scores = np.concatenate((scores_1, scores))\n",
    "    \n",
    "    preds = []\n",
    "    for i, pred in enumerate(scores):\n",
    "        r = []\n",
    "        word_ids = tokenized_datasets['test'][i]['word_id']\n",
    "        tokens = tokenized_datasets['test'][i]['tokens']\n",
    "        prev_word = None\n",
    "        for j, label in enumerate(pred):\n",
    "            if j < len(word_ids) and word_ids[j] is not None:\n",
    "                token = tokens[word_ids[j]] \n",
    "                if prev_word != word_ids[j]:\n",
    "                    r.append(label)\n",
    "                    prev_word = word_ids[j]\n",
    "        preds.append(np.array(r))\n",
    "    models_preds.append(np.array(preds))\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6614b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "result = []\n",
    "for i, j, k, p in zip(models_preds[0], models_preds[1], models_preds[2], pre_dataset):\n",
    "    h = []\n",
    "    possible_label = None\n",
    "    if i.shape != j.shape:\n",
    "        i = j\n",
    "    for a, label in enumerate(np.argmax(i + j + k, axis=1)):\n",
    "        if label == 0:\n",
    "            possible_label = p['tokens'][a]\n",
    "        elif label == 1:\n",
    "            if possible_label is None:\n",
    "                continue\n",
    "                possible_label = p['tokens'][a]\n",
    "            else: \n",
    "                possible_label = possible_label + \" \" + p['tokens'][a]\n",
    "        else:\n",
    "            if possible_label is not None:\n",
    "                possible_label = possible_label.translate(str.maketrans('', '', string.punctuation))\n",
    "                h.append(possible_label)\n",
    "                possible_label = None\n",
    "    result.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b0d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x.csv', 'w') as f:\n",
    "    for i, labels in enumerate(result):\n",
    "        f.write(str(i) + \";\" + '\"' + str(labels) + '\"'+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f821b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd00f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aspectos'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5140ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'aspectos']].to_csv('task1.csv', index=False, header=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
